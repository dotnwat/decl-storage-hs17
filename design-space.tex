\section{Design Space}
\label{sec:dspace}

The narrowly-defined interfaces representing the dominating trend in storage
systems designs is a boon allowing systems and applications to evolve
independently by establishing limitations on the size of the design space
where applications couple with storage. Programmable storage lifts the veil on
the system and, with it, forces developers of higher-level services to
confront a large and expanding set of possible designs.

In this section we highlight the size and complexity of the design space of
programmable storage. We report on our experience building multiple
functionally equivalent implementations of the CORFU protocol in Ceph, and
demonstrate that static selection of optimization strategies and tuning
decisions can lead to performance portability challenges in programmable
storage systems.

\subsection{System Tunables and Hardware}

A recent version of Ceph from May 2016 had 994 tunable parameters, controlling
all aspects of the system such as the object storage server (195), low-level
storage such as XFS or BlueStore (95), low-level sub-systems such as RocksDB
and journaling (29). Previous investigations exploring the application of
auto-tuning~\cite{behzad:sc2013-autotuning} techniques to systems exhibiting a
large space of parameters was met with limited success. And challenges
associated with this approach are exacerbated in the context of
application-specific modifications and dynamically changing workloads.

{\bf Hardware.} Ceph is intended to run on a wide variety of commodity,
high-end, and low-end hardware, including newer high-performance non-volatile
storage devices. Each hardware configuration encompasses specific sets of
performance characteristics and tunables (e.g. I/O scheduler selection, and
policies such as timeouts). In our experiments, we tested a variety of
hardware and discovered a wide range of behaviors and performance profiles.
While we generally observe the expected result of faster devices, choosing the
best implementation strategy is highly dependent on hardware. This will
continue to be true as storage systems evolve support new storage such as
persistent memories and RDMA networks that may require entirely new storage
interfaces for applications to realize fully the performance of hardware.

\textbf{Takeaway:} Evolving hardware and system tunables presents a challenge
in optimizing systems, even in static cases with fixed workloads. Programmable
storage approaches that introduce application-specific interfaces which are
sensitive to changes in workloads, and sensitive to the cost models of
low-level interfaces that are subject to change, greatly increase the design
space and set of concerns that must be addressed by programmers.

\subsection{Software}

The primary source of complexity in large storage systems is, unsurprisingly,
the vast amount of software written to handle challenges like fault-tolerance
and consistency in distributed heterogeneous environments. We have found that
even routine upgrades can cause performance regressions manifesting in obstacles 
for adopters of a programmable storage approach to development. We use the
CORFU shared-log protocol as a motivating example.

\subsubsection{Shared-log}

In our implementation of CORFU on Ceph~\cite{zlog} the shared-log is striped
across a set of objects in Ceph to provide parallel I/O bandwidth. Each object
implements a custom storage interface that exposes a 64-bit write-once address
space, and is required by the CORFU protocol.  While this interface can be
built directly into flash devices~\cite{wei:systor13}, we constructed four
different versions in software as native interfaces in Ceph. Each of our
implementations differ with respect to which internal interfaces are used
(e.g. RocksDB, and a bytestream) and how data is striped across the system.

Figure~\ref{fig:phy-design} shows the append throughput of four such
implementations running on two versions of Ceph from 2014 and 2016, in which
the performance in general is significantly better in the newer version of
Ceph. However, if we consider other costs such as software maintenace the data
reveal another trade-off. The top two performing implementations running on
the version of Ceph from 2014 perform with nearly identical throughput, but
have different implementation complexities. When we consider the performance
of the same implementations on the newer version of Ceph a challenge presents
itself: developers face a reasonable choice of a simpler implementation in the
2014 version of Ceph with little performance difference, and a storage
interface which will perform significantly worse in the 2016 version of Ceph,
requiring a significant overhaul of low-level interface implementations. We
believe that these trade-offs will continue to present themselves as new
hardware is supported and internal storage interfaces evolve.

\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{.3\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth]{jewel_v_firefly_pd.png}
        \caption{}
        %\caption{Relative performance differences can be drastic after a software
        %upgrade of the underlying storage system.}
        \label{fig:phy-design}
    \end{subfigure}
    \begin{subfigure}[b]{.3\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth]{batching.png}
        \caption{}
        %\caption{Total throughput with and without batching.}
        \label{fig:batching}
    \end{subfigure}
    \begin{subfigure}[b]{.3\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth]{batching-outlier-detect.png}
        \caption{}
        %\caption{Identifying and handling an outlier independently maintains the
        %beneifts of batching without the performance degredation of unecessarily
        %large I/O requests.}
        \label{fig:batching-outlier}
    \end{subfigure}
    \caption{(a) relative performance differences are different after storage
    software upgrade. (b) total throughput with and without batching. (c)
    identifying and handling an outlier maintains the benefits
    of batching without the performance degradation of unnecessarily large I/O
    requests.}
\end{figure*}

\subsubsection{Application-specific Group Commit}
\label{sec:batch}

In addition to the broad challenge of design and implementation, tuning
application-specific interfaces for a static implementation can be
challenging.  Group commit is a technique used in database query execution
that combines multiple transactions in order to amortize over fixed
per-transaction costs~\cite{cite} Figure~\ref{fig:batching} shows the
performance impact of using a \emph{group commit}-like technique for batching
log appends from independent clients into a single request. The
\emph{Basic-Batch} approach groups multiple requests together, but processes
each sub-request (i.e. log append) independently at the lowest level.  The
modest performance increase in this case is attributed to a reduction in
average per-request costs related to network round-trips and transaction
context management in the storage server.  Compared with the
\emph{Basic-Batch} case, the \emph{Opt-Batch} implementation is able to
achieve significantly higher performance by constructing more efficient I/O
requests using range queries and data sieving techniques~\cite{750599}
supported by the low-level I/O interfaces.

While this batching technique significantly increases throughput, the story is
more complex. The effectiveness of this technique requires tuning parameters
such as forcing request delays to achieve larger batch sizes, which in turn
have a direct effect on latency. While performance of this technique benefited
from using range queries and data sieving, these interfaces are sensitive to
outliers that generate large I/O requests containing a high percentage of
irrelevant data.  In Figure~\ref{fig:batching-outlier} the \emph{Basic-Batch}
case handles each request in a batch independently and, while the resulting
performance is worse relative to the other techniques, it is not sensitive to
outliers. The \emph{Opt-Batch} implementation achieves high append throughput,
but performance degrades as the magnitude of the outliers in the batch
increases due to wasted I/O. In contrast, an \emph{Outlier-Aware} policy
applies a simple heuristic to identify and handle outliers independently,
resulting in only a slight decrease in performance over the best case.

\textbf{Takeaway:} Choosing the best implementation of a storage interface
depends on the timing of development (e.g. system version), the expertise of
programmers and administrators, tuning parameters and hardware configuration,
as well as system-level and application-specific workload charcteristics. A
direct consequence of such a large design space is that design choices may
quickly become sub-optimal as aspects of the system change.  This forces
developers to respond increasing the risk of introducing bugs that, in the
best case, affect a single application and, in monolithic designs, are more
likely to cause systemic data loss.

We believe a better understanding of application and interface semantics
exposes a frontier of new and better approaches with more optimal maintenance
requirements than hard-coded and hand-tuned implementations. An ideal solution
to these challenges is an automated system search of
\emph{implementations}---not simply tuning parameters---based on
programmer-produced specifications of storage interfaces in a process
independent of optimization strategies and guaranteed to not introduce
correctness bugs. Next we'll discuss a candidate approach using a declarative
language for interface specification.
